{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import six\n",
    " \n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import cuda\n",
    "from chainer import serializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "    \"\"\"Neural Network definition, Multi Layer Perceptron\"\"\"\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__(\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            l1=L.Linear(None, n_units),  # n_in -> n_units\n",
    "            l2=L.Linear(None, n_units),  # n_units -> n_units\n",
    "            l3=L.Linear(None, n_out),  # n_units -> n_out\n",
    "        )\n",
    " \n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        return y\n",
    "    \n",
    "class SoftmaxClassifier(chainer.Chain):\n",
    "    \"\"\"Classifier is for calculating loss, from predictor's output.\n",
    "    predictor is a model that predicts the probability of each label.\n",
    "    \"\"\"\n",
    "    def __init__(self, predictor):\n",
    "        super(SoftmaxClassifier, self).__init__(\n",
    "            predictor=predictor\n",
    "        )\n",
    " \n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        self.loss = F.softmax_cross_entropy(y, t)\n",
    "        self.accuracy = F.accuracy(y, t)\n",
    "        return self.loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "# unit: 50\n",
      "# Minibatch-size: 100\n",
      "# epoch: 5\n",
      "out directory: result/1_minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz...\n",
      "Downloading from http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz...\n",
      "Downloading from http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "train mean loss=0.4122396842390299, accuracy=0.8847666671623786, throughput=23543.566333568622 images/sec\n",
      "test  mean loss=0.21441456601954997, accuracy=0.9381000036001206\n",
      "epoch 2\n",
      "train mean loss=0.18375116545086106, accuracy=0.9464166672031085, throughput=26435.9914369731 images/sec\n",
      "test  mean loss=0.16052430846495555, accuracy=0.9517000043392181\n",
      "epoch 3\n",
      "train mean loss=0.1423880790049831, accuracy=0.9576166693369548, throughput=23999.558266187156 images/sec\n",
      "test  mean loss=0.13983906885492614, accuracy=0.9586000061035156\n",
      "epoch 4\n",
      "train mean loss=0.11849292076192797, accuracy=0.9648166718085607, throughput=25041.475544886755 images/sec\n",
      "test  mean loss=0.12401987234363332, accuracy=0.963400005698204\n",
      "epoch 5\n",
      "train mean loss=0.10147439869431157, accuracy=0.9689500072598457, throughput=24344.289436530136 images/sec\n",
      "test  mean loss=0.11619650734704919, accuracy=0.9654000067710876\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Configuration setting\n",
    "    gpu = -1                  # GPU ID to be used for calculation. -1 indicates to use only CPU.\n",
    "    batchsize = 100           # Minibatch size for training\n",
    "    epoch = 5                # Number of training epoch\n",
    "    out = 'result/1_minimum'  # Directory to save the results\n",
    "    unit = 50                 # Number of hidden layer units, try incresing this value and see if how accuracy changes.\n",
    " \n",
    "    print('GPU: {}'.format(gpu))\n",
    "    print('# unit: {}'.format(unit))\n",
    "    print('# Minibatch-size: {}'.format(batchsize))\n",
    "    print('# epoch: {}'.format(epoch))\n",
    "    print('out directory: {}'.format(out))\n",
    " \n",
    "    # Set up a neural network to train\n",
    "    model = MLP(unit, 10)\n",
    "    # Classifier will calculate classification loss, based on the output of model\n",
    "    classifier_model = SoftmaxClassifier(model)\n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()  # Make a specified GPU current\n",
    "        classifier_model.to_gpu()           # Copy the model to the GPU\n",
    "    xp = np if gpu < 0 else cuda.cupy\n",
    "    # Setup an optimizer\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(classifier_model)\n",
    " \n",
    "    # Load the MNIST dataset\n",
    "    train, test = chainer.datasets.get_mnist()\n",
    " \n",
    "    n_epoch = epoch\n",
    "    N = len(train)       # training data size\n",
    "    N_test = len(test)  # test data size\n",
    " \n",
    "    # Learning loop\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print('epoch', epoch)\n",
    " \n",
    "        # training\n",
    "        perm = np.random.permutation(N)\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        start = time.time()\n",
    "        for i in six.moves.range(0, N, batchsize):\n",
    "            x = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][0]))\n",
    "            t = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][1]))\n",
    " \n",
    "            # Pass the loss function (Classifier defines it) and its arguments\n",
    "            optimizer.update(classifier_model, x, t)\n",
    " \n",
    "            sum_loss += float(classifier_model.loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        throughput = N / elapsed_time\n",
    "        print('train mean loss={}, accuracy={}, throughput={} images/sec'.format(\n",
    "            sum_loss / N, sum_accuracy / N, throughput))\n",
    " \n",
    "        # evaluation\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        for i in six.moves.range(0, N_test, batchsize):\n",
    "            index = np.asarray(list(range(i, i + batchsize)))\n",
    "            x = chainer.Variable(xp.asarray(test[index][0]))\n",
    "            t = chainer.Variable(xp.asarray(test[index][1]))\n",
    " \n",
    "            loss = classifier_model(x, t)\n",
    "            sum_loss += float(loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    " \n",
    "        print('test  mean loss={}, accuracy={}'.format(\n",
    "            sum_loss / N_test, sum_accuracy / N_test))\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing number of hidden layers from 50 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Configuration setting\n",
    "    gpu = -1                  # GPU ID to be used for calculation. -1 indicates to use only CPU.\n",
    "    batchsize = 100           # Minibatch size for training\n",
    "    epoch = 5                # Number of training epoch\n",
    "    out = 'result/1_minimum'  # Directory to save the results\n",
    "    unit = 100                 # Number of hidden layer units, try incresing this value and see if how accuracy changes.\n",
    " \n",
    "    print('GPU: {}'.format(gpu))\n",
    "    print('# unit: {}'.format(unit))\n",
    "    print('# Minibatch-size: {}'.format(batchsize))\n",
    "    print('# epoch: {}'.format(epoch))\n",
    "    print('out directory: {}'.format(out))\n",
    " \n",
    "    # Set up a neural network to train\n",
    "    model = MLP(unit, 10)\n",
    "    # Classifier will calculate classification loss, based on the output of model\n",
    "    classifier_model = SoftmaxClassifier(model)\n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()  # Make a specified GPU current\n",
    "        classifier_model.to_gpu()           # Copy the model to the GPU\n",
    "    xp = np if gpu < 0 else cuda.cupy\n",
    "    # Setup an optimizer\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(classifier_model)\n",
    " \n",
    "    # Load the MNIST dataset\n",
    "    train, test = chainer.datasets.get_mnist()\n",
    " \n",
    "    n_epoch = epoch\n",
    "    N = len(train)       # training data size\n",
    "    N_test = len(test)  # test data size\n",
    " \n",
    "    # Learning loop\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print('epoch', epoch)\n",
    " \n",
    "        # training\n",
    "        perm = np.random.permutation(N)\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        start = time.time()\n",
    "        for i in six.moves.range(0, N, batchsize):\n",
    "            x = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][0]))\n",
    "            t = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][1]))\n",
    " \n",
    "            # Pass the loss function (Classifier defines it) and its arguments\n",
    "            optimizer.update(classifier_model, x, t)\n",
    " \n",
    "            sum_loss += float(classifier_model.loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        throughput = N / elapsed_time\n",
    "        print('train mean loss={}, accuracy={}, throughput={} images/sec'.format(\n",
    "            sum_loss / N, sum_accuracy / N, throughput))\n",
    " \n",
    "        # evaluation\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        for i in six.moves.range(0, N_test, batchsize):\n",
    "            index = np.asarray(list(range(i, i + batchsize)))\n",
    "            x = chainer.Variable(xp.asarray(test[index][0]))\n",
    "            t = chainer.Variable(xp.asarray(test[index][1]))\n",
    " \n",
    "            loss = classifier_model(x, t)\n",
    "            sum_loss += float(loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    " \n",
    "        print('test  mean loss={}, accuracy={}'.format(\n",
    "            sum_loss / N_test, sum_accuracy / N_test))\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing number of hidden layers from 100 to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "# unit: 500\n",
      "# Minibatch-size: 100\n",
      "# epoch: 5\n",
      "out directory: result/1_minimum\n",
      "epoch 1\n",
      "train mean loss=0.22122278173143664, accuracy=0.9345166688164075, throughput=769.0304063994142 images/sec\n",
      "test  mean loss=0.11015882198233157, accuracy=0.9680000030994416\n",
      "epoch 2\n",
      "train mean loss=0.08005895735773569, accuracy=0.9748333435257276, throughput=567.8751289323735 images/sec\n",
      "test  mean loss=0.08384176248218864, accuracy=0.973300005197525\n",
      "epoch 3\n",
      "train mean loss=0.05355364949287226, accuracy=0.9828666772445043, throughput=552.7098209462644 images/sec\n",
      "test  mean loss=0.07390523866924922, accuracy=0.977000008225441\n",
      "epoch 4\n",
      "train mean loss=0.0374969503415438, accuracy=0.9879000095526377, throughput=545.403833217965 images/sec\n",
      "test  mean loss=0.07387393044031342, accuracy=0.9790000063180924\n",
      "epoch 5\n",
      "train mean loss=0.028722861313047663, accuracy=0.9907166745265324, throughput=469.24537218152307 images/sec\n",
      "test  mean loss=0.07021083826381073, accuracy=0.9794000071287156\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Configuration setting\n",
    "    gpu = -1                  # GPU ID to be used for calculation. -1 indicates to use only CPU.\n",
    "    batchsize = 100           # Minibatch size for training\n",
    "    epoch = 5                # Number of training epoch\n",
    "    out = 'result/1_minimum'  # Directory to save the results\n",
    "    unit = 500                # Number of hidden layer units, try incresing this value and see if how accuracy changes.\n",
    " \n",
    "    print('GPU: {}'.format(gpu))\n",
    "    print('# unit: {}'.format(unit))\n",
    "    print('# Minibatch-size: {}'.format(batchsize))\n",
    "    print('# epoch: {}'.format(epoch))\n",
    "    print('out directory: {}'.format(out))\n",
    " \n",
    "    # Set up a neural network to train\n",
    "    model = MLP(unit, 10)\n",
    "    # Classifier will calculate classification loss, based on the output of model\n",
    "    classifier_model = SoftmaxClassifier(model)\n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()  # Make a specified GPU current\n",
    "        classifier_model.to_gpu()           # Copy the model to the GPU\n",
    "    xp = np if gpu < 0 else cuda.cupy\n",
    "    # Setup an optimizer\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(classifier_model)\n",
    " \n",
    "    # Load the MNIST dataset\n",
    "    train, test = chainer.datasets.get_mnist()\n",
    " \n",
    "    n_epoch = epoch\n",
    "    N = len(train)       # training data size\n",
    "    N_test = len(test)  # test data size\n",
    " \n",
    "    # Learning loop\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print('epoch', epoch)\n",
    " \n",
    "        # training\n",
    "        perm = np.random.permutation(N)\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        start = time.time()\n",
    "        for i in six.moves.range(0, N, batchsize):\n",
    "            x = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][0]))\n",
    "            t = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][1]))\n",
    " \n",
    "            # Pass the loss function (Classifier defines it) and its arguments\n",
    "            optimizer.update(classifier_model, x, t)\n",
    " \n",
    "            sum_loss += float(classifier_model.loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        throughput = N / elapsed_time\n",
    "        print('train mean loss={}, accuracy={}, throughput={} images/sec'.format(\n",
    "            sum_loss / N, sum_accuracy / N, throughput))\n",
    " \n",
    "        # evaluation\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        for i in six.moves.range(0, N_test, batchsize):\n",
    "            index = np.asarray(list(range(i, i + batchsize)))\n",
    "            x = chainer.Variable(xp.asarray(test[index][0]))\n",
    "            t = chainer.Variable(xp.asarray(test[index][1]))\n",
    " \n",
    "            loss = classifier_model(x, t)\n",
    "            sum_loss += float(loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    " \n",
    "        print('test  mean loss={}, accuracy={}'.format(\n",
    "            sum_loss / N_test, sum_accuracy / N_test))\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
