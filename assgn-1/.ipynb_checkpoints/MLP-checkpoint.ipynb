{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d0fd2af4bfc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chainer'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import six\n",
    " \n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import cuda\n",
    "from chainer import serializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "    \"\"\"Neural Network definition, Multi Layer Perceptron\"\"\"\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__(\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            l1=L.Linear(None, n_units),  # n_in -> n_units\n",
    "            l2=L.Linear(None, n_units),  # n_units -> n_units\n",
    "            l3=L.Linear(None, n_out),  # n_units -> n_out\n",
    "        )\n",
    " \n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        return y\n",
    "    \n",
    "class SoftmaxClassifier(chainer.Chain):\n",
    "    \"\"\"Classifier is for calculating loss, from predictor's output.\n",
    "    predictor is a model that predicts the probability of each label.\n",
    "    \"\"\"\n",
    "    def __init__(self, predictor):\n",
    "        super(SoftmaxClassifier, self).__init__(\n",
    "            predictor=predictor\n",
    "        )\n",
    " \n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        self.loss = F.softmax_cross_entropy(y, t)\n",
    "        self.accuracy = F.accuracy(y, t)\n",
    "        return self.loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Configuration setting\n",
    "    gpu = -1                  # GPU ID to be used for calculation. -1 indicates to use only CPU.\n",
    "    batchsize = 100           # Minibatch size for training\n",
    "    epoch = 5                # Number of training epoch\n",
    "    out = 'result/1_minimum'  # Directory to save the results\n",
    "    unit = 50                 # Number of hidden layer units, try incresing this value and see if how accuracy changes.\n",
    " \n",
    "    print('GPU: {}'.format(gpu))\n",
    "    print('# unit: {}'.format(unit))\n",
    "    print('# Minibatch-size: {}'.format(batchsize))\n",
    "    print('# epoch: {}'.format(epoch))\n",
    "    print('out directory: {}'.format(out))\n",
    " \n",
    "    # Set up a neural network to train\n",
    "    model = MLP(unit, 10)\n",
    "    # Classifier will calculate classification loss, based on the output of model\n",
    "    classifier_model = SoftmaxClassifier(model)\n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()  # Make a specified GPU current\n",
    "        classifier_model.to_gpu()           # Copy the model to the GPU\n",
    "    xp = np if gpu < 0 else cuda.cupy\n",
    "    # Setup an optimizer\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(classifier_model)\n",
    " \n",
    "    # Load the MNIST dataset\n",
    "    train, test = chainer.datasets.get_mnist()\n",
    " \n",
    "    n_epoch = epoch\n",
    "    N = len(train)       # training data size\n",
    "    N_test = len(test)  # test data size\n",
    " \n",
    "    # Learning loop\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print('epoch', epoch)\n",
    " \n",
    "        # training\n",
    "        perm = np.random.permutation(N)\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        start = time.time()\n",
    "        for i in six.moves.range(0, N, batchsize):\n",
    "            x = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][0]))\n",
    "            t = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][1]))\n",
    " \n",
    "            # Pass the loss function (Classifier defines it) and its arguments\n",
    "            optimizer.update(classifier_model, x, t)\n",
    " \n",
    "            sum_loss += float(classifier_model.loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        throughput = N / elapsed_time\n",
    "        print('train mean loss={}, accuracy={}, throughput={} images/sec'.format(\n",
    "            sum_loss / N, sum_accuracy / N, throughput))\n",
    " \n",
    "        # evaluation\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        for i in six.moves.range(0, N_test, batchsize):\n",
    "            index = np.asarray(list(range(i, i + batchsize)))\n",
    "            x = chainer.Variable(xp.asarray(test[index][0]))\n",
    "            t = chainer.Variable(xp.asarray(test[index][1]))\n",
    " \n",
    "            loss = classifier_model(x, t)\n",
    "            sum_loss += float(loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    " \n",
    "        print('test  mean loss={}, accuracy={}'.format(\n",
    "            sum_loss / N_test, sum_accuracy / N_test))\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing number of hidden layers from 50 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Configuration setting\n",
    "    gpu = -1                  # GPU ID to be used for calculation. -1 indicates to use only CPU.\n",
    "    batchsize = 100           # Minibatch size for training\n",
    "    epoch = 5                # Number of training epoch\n",
    "    out = 'result/1_minimum'  # Directory to save the results\n",
    "    unit = 100                 # Number of hidden layer units, try incresing this value and see if how accuracy changes.\n",
    " \n",
    "    print('GPU: {}'.format(gpu))\n",
    "    print('# unit: {}'.format(unit))\n",
    "    print('# Minibatch-size: {}'.format(batchsize))\n",
    "    print('# epoch: {}'.format(epoch))\n",
    "    print('out directory: {}'.format(out))\n",
    " \n",
    "    # Set up a neural network to train\n",
    "    model = MLP(unit, 10)\n",
    "    # Classifier will calculate classification loss, based on the output of model\n",
    "    classifier_model = SoftmaxClassifier(model)\n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()  # Make a specified GPU current\n",
    "        classifier_model.to_gpu()           # Copy the model to the GPU\n",
    "    xp = np if gpu < 0 else cuda.cupy\n",
    "    # Setup an optimizer\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(classifier_model)\n",
    " \n",
    "    # Load the MNIST dataset\n",
    "    train, test = chainer.datasets.get_mnist()\n",
    " \n",
    "    n_epoch = epoch\n",
    "    N = len(train)       # training data size\n",
    "    N_test = len(test)  # test data size\n",
    " \n",
    "    # Learning loop\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print('epoch', epoch)\n",
    " \n",
    "        # training\n",
    "        perm = np.random.permutation(N)\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        start = time.time()\n",
    "        for i in six.moves.range(0, N, batchsize):\n",
    "            x = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][0]))\n",
    "            t = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][1]))\n",
    " \n",
    "            # Pass the loss function (Classifier defines it) and its arguments\n",
    "            optimizer.update(classifier_model, x, t)\n",
    " \n",
    "            sum_loss += float(classifier_model.loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        throughput = N / elapsed_time\n",
    "        print('train mean loss={}, accuracy={}, throughput={} images/sec'.format(\n",
    "            sum_loss / N, sum_accuracy / N, throughput))\n",
    " \n",
    "        # evaluation\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        for i in six.moves.range(0, N_test, batchsize):\n",
    "            index = np.asarray(list(range(i, i + batchsize)))\n",
    "            x = chainer.Variable(xp.asarray(test[index][0]))\n",
    "            t = chainer.Variable(xp.asarray(test[index][1]))\n",
    " \n",
    "            loss = classifier_model(x, t)\n",
    "            sum_loss += float(loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    " \n",
    "        print('test  mean loss={}, accuracy={}'.format(\n",
    "            sum_loss / N_test, sum_accuracy / N_test))\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing number of hidden layers from 100 to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Configuration setting\n",
    "    gpu = -1                  # GPU ID to be used for calculation. -1 indicates to use only CPU.\n",
    "    batchsize = 100           # Minibatch size for training\n",
    "    epoch = 5                # Number of training epoch\n",
    "    out = 'result/1_minimum'  # Directory to save the results\n",
    "    unit = 500                # Number of hidden layer units, try incresing this value and see if how accuracy changes.\n",
    " \n",
    "    print('GPU: {}'.format(gpu))\n",
    "    print('# unit: {}'.format(unit))\n",
    "    print('# Minibatch-size: {}'.format(batchsize))\n",
    "    print('# epoch: {}'.format(epoch))\n",
    "    print('out directory: {}'.format(out))\n",
    " \n",
    "    # Set up a neural network to train\n",
    "    model = MLP(unit, 10)\n",
    "    # Classifier will calculate classification loss, based on the output of model\n",
    "    classifier_model = SoftmaxClassifier(model)\n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()  # Make a specified GPU current\n",
    "        classifier_model.to_gpu()           # Copy the model to the GPU\n",
    "    xp = np if gpu < 0 else cuda.cupy\n",
    "    # Setup an optimizer\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(classifier_model)\n",
    " \n",
    "    # Load the MNIST dataset\n",
    "    train, test = chainer.datasets.get_mnist()\n",
    " \n",
    "    n_epoch = epoch\n",
    "    N = len(train)       # training data size\n",
    "    N_test = len(test)  # test data size\n",
    " \n",
    "    # Learning loop\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print('epoch', epoch)\n",
    " \n",
    "        # training\n",
    "        perm = np.random.permutation(N)\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        start = time.time()\n",
    "        for i in six.moves.range(0, N, batchsize):\n",
    "            x = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][0]))\n",
    "            t = chainer.Variable(xp.asarray(train[perm[i:i + batchsize]][1]))\n",
    " \n",
    "            # Pass the loss function (Classifier defines it) and its arguments\n",
    "            optimizer.update(classifier_model, x, t)\n",
    " \n",
    "            sum_loss += float(classifier_model.loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        throughput = N / elapsed_time\n",
    "        print('train mean loss={}, accuracy={}, throughput={} images/sec'.format(\n",
    "            sum_loss / N, sum_accuracy / N, throughput))\n",
    " \n",
    "        # evaluation\n",
    "        sum_accuracy = 0\n",
    "        sum_loss = 0\n",
    "        for i in six.moves.range(0, N_test, batchsize):\n",
    "            index = np.asarray(list(range(i, i + batchsize)))\n",
    "            x = chainer.Variable(xp.asarray(test[index][0]))\n",
    "            t = chainer.Variable(xp.asarray(test[index][1]))\n",
    " \n",
    "            loss = classifier_model(x, t)\n",
    "            sum_loss += float(loss.data) * len(t.data)\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(t.data)\n",
    " \n",
    "        print('test  mean loss={}, accuracy={}'.format(\n",
    "            sum_loss / N_test, sum_accuracy / N_test))\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
